\documentclass[a4paper,english,numbers=noenddot,bibliography=totoc,chapterprefix=on,DIV=12]{scrartcl}

\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amssymb,amsmath}
\usepackage{enumitem}
\PassOptionsToPackage{hyphens}{url}\usepackage{bookmark,hyperref}
\usepackage{multirow}
\usepackage[labelfont=bf,font=small]{caption}
\usepackage[font=footnotesize]{subcaption}
\usepackage{rotating}

\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{listings}

\setcapindent{0mm}

% boldmath in headings and toc, but not headers
\def\bfseries{\fontseries \bfdefault \selectfont \boldmath}

%allows footnotes in tabular
\usepackage{footnote}
\makesavenoteenv{tabular}
\makesavenoteenv{table}

% avoid space issues after macro
\usepackage{xspace}
\newcommand{\tof}{TOF\xspace}
\newcommand{\angstrom}{\textup{\AA}}

\newcommand{\Treduction}{t_{\text{reduction}}}
\newcommand{\Trun}{t_{\text{run}}}
\newcommand{\Tbin}{t_{\text{bin}}}
\newcommand{\Tevent}{t_{\text{event}}}
\newcommand{\Nbin}{N_{\text{bin}}}
\newcommand{\Ncore}{N_{\text{core}}}
\newcommand{\Nevent}{N_{\text{event}}}
\newcommand{\Nreduction}{N_{\text{reduction}}}
\newcommand{\Nspec}{N_{\text{spec}}}
\newcommand{\Bmax}{B_{\text{max}}}
\newcommand{\Fevent}{f_{\text{event}}}

\newcommand{\beer}{BEER\xspace}
\newcommand{\bifrost}{BIFROST\xspace}
\newcommand{\cspec}{CSPEC\xspace}
\newcommand{\dream}{DREAM\xspace}
\newcommand{\estia}{ESTIA\xspace}
\newcommand{\loki}{LoKI\xspace}
\newcommand{\magic}{MAGIC\xspace}
\newcommand{\odin}{ODIN\xspace}

\newcommand{\nexus}{NeXus\xspace}

\begin{document}

\title{Estimation of hardware requirements for data reduction with Mantid at ESS}
\author{Simon Heybrock, Lamar Moore, Neil Vaytet\\
    {\small\href{mailto:simon.heybrock@esss.se}{\nolinkurl{simon.heybrock@esss.se}}}}

\maketitle

\tableofcontents

effects of wrong estimations for certain parameters, e.g., what happens if actual event rate is 2x higher/lower (would it be useful to tread all parameters as random values with a distribution of a certain width, such that we can do some sampling to obtain error bars?)
discuss strong influence of run duration (and the ratio of events to pixel count where either contribution becomes dominant, discuss grouping of pixels, in particular for 3D detectors?)

\section{Introduction}

The aim of this document is to capture our understanding of what hardware resources will be required for reducing data with Mantid for ESS.
While we attempt to give some estimates for, e.g., required core counts, it will become clear below there are too many uncertainties. In practice the main contribution of this document may thus be the description of how various parameters affect what hardware is required, allowing us the continuously update and improve our estimates.

Data reduction covers three main cases, live reduction, interactive reduction, and script-based reduction.

\paragraph{Live reduction}
Preliminary/simplified on-the-fly reduction of data and visualization is a key promise of ESS and will be required for all instruments, any time the beam is on and at times probably also with beam off for other calibration work.
It is critical that this is available at all times so a dedicated pool of hardware may be required.

\paragraph{Interactive reduction} 
The general view is that data at ESS will be so large that users cannot do data reduction on their laptop or desktop PC. Therefore DMSC needs to provide resources for all interactive workflows using the Mantid GUI (MantidPlot or its successor) for data reduction and visualization.
This is required during a user visit as well as a certain time period after the experiment.
Furthermore, we need to provide access to such resources \emph{before} an experiment such that users can familiarize themselves with the tools and provided software infrastructure, avoiding wasting beam time due to software problems.
For interactive reduction, due to the interactive nature, high CPU use will alternate with shorter or longer idle breaks.
However, generally there will be a rather large permanent memory requirement, i.e., clustering interactive user sessions on a single compute node is only possible to a limited extent, and will require nodes with a large amount of memory.

\paragraph{Script-based reduction}
Reduction of data based on a (Python) script.
This covers automatic reduction and batch reduction.
This is suitable for being run on a cluster using a standard queuing system with adequate means to ensure that queue lengths stay within limits.
For example, one could imagine that jobs for data reduction of a running experiment need to be prioritized over modelling jobs of a experiment that happened a long time ago.

\subsection{Threading and MPI support}
\label{sec:threading-and-mpi-support}

The Mantid framework is multithreaded and algorithms run by a script or interactively by the GUI will typically run with multiple threads.
To allow for scaling data reduction beyond a single node MPI support has been added to Mantid.
MPI support is only available for script-based reduction.
In principle it could also be used for live reduction, provided that it is only a passive way to observe raw and reduced data.
There is no MPI support connected to the GUI and there are no plans for this either.\footnote{That is, there will be no equivalent to the client-server MPI mode supported by, e.g., ParaView}.

In various benchmarks we observed that generally an equivalent threaded run of a Mantid reduction script takes longer than the same reduction done with MPI:\footnote{This is true even without the optimized MPI-based NeXus loader.}
The scaling with the number of used cores is worse.
The reason for this is not fully understood but it is probably related to the fork-join threading approach.
Thus we cannot directly transfer results and scaling behaviors obtained for MPI to a threaded version of Mantid.
For machines with a moderate number of cores of up to roughly 10 cores we can probably use the rule of thumb that the threaded reduction will be $2\times$ slower than the MPI reduction.

\subsection{Event filtering}

Many ESS instruments will require filtering events in one way or another.
Benchmarks indicate that the contribution of this is to the overall runtime is less than $10\%$, i.e., we do not need to handle it explicitly in our evaluations.


\section{Scaling analysis}

The number of cores required for reduction depends on the required maximum runtime for a reduction.
It is unclear what a good limit for the runtime $\Treduction$ is.
For now we define:

\begin{itemize}
  \item The reduction should be $5\times$ faster than the experiment.
  \item To ensure that long-running experiments with low event rates are processed quickly we require that a minimum of $2\cdot10^6$ events/s are processed. TODO this does not really work and runs into trouble for instruments with many pixels, need a better definition.
  \item We never require a runtime below 30 seconds.
\end{itemize}
A visualization of these requirements is given in Fig.~\ref{fig:reduction-time-requirement}.

\begin{figure}
  \centering
\includegraphics[height=0.33\textheight]{reduction-time-requirement.pdf}
\caption{\label{fig:reduction-time-requirement}Imposed requirements for the maximum reduction time. The checked areas indicate regions that are unreachable due to the defined limits. For runs that are short relative to the event rate we are governed by the 30-second limit. Above that limit, at low event rates the required reduction duration is low compared to the run duration. With increasing event rate we get closer to the limit of $5\times$ speedup indicated by the bold red line.}
\end{figure}

With a couple of approximations (which are probably minor for this purpose and compared to other sources of uncertainty) we can describe the time required to reduce a set of data as

\begin{align}
  \Treduction = t_0 + \frac{\Nbin\Tbin + \Nevent\Tevent}{\Ncore} + \frac{\Nevent}{\Bmax},
  \label{eq:master}
\end{align}
where
\begin{itemize}
  \item $\Nbin$ is the number of bins in the workflow, i.e., the number of spectra $\Nspec$ multiplied by the number of bins per spectrum.\footnote{Typically $\Nspec$ is the number of pixels of the instrument, but it can be different, e.g., when data from pixels is split up, such as for event filtering or RRM.}
    The number of bins per spectrum depends on the bandwidth and resolution of the instrument.
    As a rule of thumb, for a given energy resolution $\Delta E$ we require a bin size of $\Delta E/10$.
  \item $\Nevent$ is the total number of events that are being handled in the reduction workflow.
    This can include events from multiple files, e.g., for a sample run and a background run.
  \item $\Ncore$ is the number of cores (MPI ranks) used in the reduction.
    We are not considering a hybrid threading+MPI approach (apart from a few specialized algorithms that are using threading internally).
  \item $t_0$ is a constant time specific to the reduction workflow.
    It includes anything that does not depend and the number of spectra or number of events.
    Typically this includes small parts of the time spend in every algorithm, time for loading experiment logs from NeXus files, time for loading auxiliary files, and other overheads.
  \item $\Tbin$ is the (computed) time to run the workflow for a single bin.
  \item $\Tevent$ is the (computed) time to run the workflow for a single event.
  \item $\Bmax$ is the number of events that can be loaded from the file system per second.
\end{itemize}
The equation is motivated by an analysis of benchmark results (see Sec.~\ref{sec:benchmarks}) and the computational structure of data reduction.
$\Ncore$ can be adjusted such that $\Treduction$ fulfills the conditions listed above.
There can be cases where the conditions are violated, e.g., when the event rate is high and $\Bmax$ is too low.
More details on this equation can be found in App. \ref{app:master_eq}.

Our benchmarks based on existing instruments and investigations of parameters for a series of ESS instruments show that any of the terms in Eq.~\eqref{eq:master} can be relevant or even dominant, depending on the instrument, the experiment, and the number of used cores.

\begin{itemize}
  \item 
    For instruments that produce many small files (few spectra and events), the $t_0$ term can become dominant.
    This can in theory be improved by processing multiple files in the same workflow, but at this point making such an assumption is not justified.
    It is thus important to capture the typical run duration for each instrument.
  \item
    Even if the $t_0$ term is not dominant, a run may contain relatively few events relative to the pixel count.
    In that case the $\Nbin$ term is important.
    Given that many ESS instruments have a high pixel count this could very well become a dominating factor.\footnote{In many experiments the total number of events required for each measurement may not increase over present-day experiments. However, the ESS event \emph{rate} and pixel counts are higher, so the balance may shift to our disadvantage.}
  \item
    For instruments with many events we may be using many cores to offset the reduction cost.
    In that limit the bandwidth-limiting term becomes relevant.
\end{itemize}

Based on the time for reducing a single run we can thus compute the average number of cores required for reducing data for the instrument.
Note that this does \emph{not} include live reduction and interactive sessions.

\begin{align}
  \overline{\Ncore} = \Nreduction \Ncore \frac{\Treduction}{\Trun}.
\end{align}

Here:

\begin{itemize}
  \item $\Nreduction$ is the number of times data is reduced.
    Typically this should be small, e.g., 1, 2, or 3, but especially in the early days there will be exceptions.
  \item $\Trun$ is the duration of a single run.
\end{itemize}

For convenience, we can expand the master equation and obtain

\begin{align}
  \overline{\Ncore} &= \frac{\Nreduction}{\Trun}\left[\Ncore \left(t_0 + \frac{\Nevent}{\Bmax}\right) + \Nbin\Tbin + \Nevent\Tevent\right]\\
   &= \Nreduction\left[\Ncore \left(\frac{t_0}{\Trun} + \frac{\Fevent}{\Bmax}\right) + \frac{\Nbin\Tbin}{\Trun} + \Fevent\Tevent\right]\\
   &= \Nreduction\left[\Ncore \left(\frac{t_0\Fevent}{\Nevent} + \frac{\Fevent}{\Bmax}\right) + \frac{\Nbin\Tbin\Fevent}{\Nevent} + \Fevent\Tevent\right].
\end{align}
All three lines are simple variations of each other and are merely listed to highlight different aspects of the scaling behavior.

It is important to note that $\overline{\Ncore}$ depends on $\Ncore$, i.e., the more cores we use, the higher our overall hardware requirement.
Reasons for using more cores are primarily to (1) reduce the time for a single reduction to something that is acceptable for users, and (2) work around memory limitations on a single node.
Furthermore, there are two terms that depend on $1/\Trun$, i.e., shorter runs will increase the number of required cores even if all other parameters such as the event rate $\Fevent$ are unchanged.

\section{Input parameters}

In Fig.~\ref{fig:accelerator-power} we show the estimate for the accelerator power ramp-up used in the following for scaling the expected event rates.

\begin{figure}
  \centering
\includegraphics[height=0.33\textheight]{accelerator-power.pdf}
\caption{\label{fig:accelerator-power}Estimate of accelerator power ramp up. ESS instrument scientists expect a linear scaling of the useable wavelength with the accelerator power.}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{lrrl}
    Instrument & Pixel count day 1 & Pixel count final & Comment\\
    \hline
    \beer & 200k & 400k & not including SANS upgrade\\
    \bifrost & ? & 5k & low pixel count but scanning\\
    \cspec & 400k & 750k \\
    % 1.25*32*6.2/(0.003*0.006) = 14000k
    \dream & 4000 & 12000k & estimated\footnote{We did not yet receive pixel counts from DREAM, but MAGIC uses same technology with a pixel size of $3~\mathrm{mm} \times 6~\mathrm{mm}$ with 32 voxel layers. Full is $5.11~\mathrm{sr}$ with a sample-detector distance of $1.25~\mathrm{m}$. Day-1 coverage is $1.81~\mathrm{sr}$.}\\
    \estia & 250k & 500k \\
    \loki & 750k & 1500k \\
    \magic & 1440k & 2880k \\
    \odin & ? & ? & imaging is special case, handle separately?\\
  \end{tabular}
  \caption{\label{tab:pixel-counts}Pixel counts for the first eight instruments. Generally instruments to not have full detector coverage at day 1 and will add more pixels in an upgrade, typically a couple of years later.}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{llrrr}
    Instrument & Configuration & use fraction & event rate & total event count \\
    \hline
    \hline
    \multirow{6}{*}{\loki}
    & 3m-high-flux    & 0.1 & $1.0\cdot10^7$ & $10^8$ \\
    & 3m-small-sample & 0.1 & $3.33\cdot10^5$ & $10^8$ \\
    & 5m-high-flux    & 0.1 & $1.92\cdot10^6$ & $10^8$ \\
    & 5m-small-sample & 0.3 & $1.2\cdot10^5$ & $10^8$ \\
    & 8m-high-flux    & 0.1 & $7.5\cdot10^5$ & $10^8$ \\
    & 8m-small-sample & 0.3 & $4.69\cdot10^4$ & $10^8$ \\
    \hline
    \multirow{5}{*}{\estia} & reference-high-intensity\\ & reference-normal\\ & specular-high-intensity\\ & specular\\ & off-specular\\
  \end{tabular}
  \caption{\label{tab:configurations}Instrument configurations with expected event rates at full power and detector build-out.}
\end{table}


\section{Baseline benchmarks}
\label{sec:benchmarks}

To establish an estimate for the parameters $t_0$, $\Tbin$, $\Tevent$, and $\Bmax$ in Eq.~\eqref{eq:master} a number of benchmarks have been undertaken.

\begin{enumerate}
  \item Using an MPI-build of Mantid we measured run times of workflows for SANS, powder diffraction, and direct geometry spectroscopy (using the ISIS workflow for SANS2D, the \verb|SNSPowderDiffraction| workflow for PG3, and the \verb|DgsReduction| workflow for CNCS).
    We studied the scaling behavior with the number of used cores (MPI ranks), the number of events, and also the number of bins per spectrum.
    By looking at the various limits (few/many cores and few/many events) we can extract an approximation for $t_0$, $\Tbin$, $\Tevent$.
    All these experiments were done on a single node, a dual-socket Xeon E5-2620 v3 running at 2.40GHz.
  \item Scaling beyond a single node has been established in older benchmarks in 2015, again using the \verb|SNSPowderDiffraction|.
    This benchmark showed that scaling beyond a single node does not lead to any significant slowdown and scaling works well up to a high number of MPI ranks.
  \item For the special case of I/O we rely on benchmarks for the recently optimized parallel event loader for \nexus files.
    Currently only benchmarks with a local SSD are available and show scaling up to nearly $10^8$ loaded events per second (slightly lower for compressed files) with 10 used cores.
    Benchmarks with a parallel file system are pending.
\end{enumerate}
The experiments show a surprisingly consistent pattern, independent of the technique:

\begin{itemize}
\item $t_0$ is about 10 seconds.
\item $\Tbin$ varies from 1/1.000.000 seconds for histogram-heavy reductions to about 1/10.000.000 seconds for reductions with near-ubiquitous use of event-mode.
\item $\Tevent$ is about 1/1.000.000 seconds, or slightly smaller.
\item $\Bmax$ is about 50.000.000 events/second for an SSD, tests with a parallel file system are pending.
\end{itemize}


\section{Results}

Example core counts as well as average core counts are given in Tab.~\ref{}.
Provided that the compute cluster is hard enough summing the average core counts could yield the required size of the compute cluster.
However, there are clearly cases with high peak-core requirements.
Furthermore, in the early days with few operating instruments and a smaller cluster it needs to be ensured that an appropriate partition is always available within a to be defined time frame.
Determining the resulting required size of the compute cluster is beyond the scope of this investigation.

\subsection{Other factors}

With low accelerator power and lower detector coverage in the early it is to be expected that the type of experiments will be different from what is envisioned for full power.
For example, larger samples will be used to compensate for the lower rates.
The compensation could be of up to a order of magnitude in size, at least for certain experiments.
While some instruments indicate they will do so, e.g., \loki and \dream, not all of them will, e.g., \cspec.
We have \emph{not} included a scaling term to represent this effect in the numbers given in this document.
Ensuring that the compute cluster size ramp up is sufficiently ahead of the accelerator power ramp up could help to deal with this uncertainty.

\section{Live reduction}

In Tab.~\ref{tab:consumption-rate} we show benchmark results for the event consumption rate of the Kafka event listener in Mantid without processing or visualization).
This test is done using a default (non-MPI) build of Mantid.
MPI support for the Kafka listener is not implemented currently.

With the exception of the SANS2D data point at $10^5$ the scaling with the event rate is linear.\footnote{It is likely that for the high rate of more than 1000 frames per second other effects become relevant. Since we are not interested in such high frame rates we ignore this data point and assume linear scaling.}
The benchmarks also show a (very approximately) linear scaling of the consumption rate with the number of pixels.
This is not entirely expected.
There are two aspects that can be used to explain the scaling:
With more pixels that write behavior to the \verb|EventWorkspace| becomes more random leading to a less and less efficient use of cache and the rest of the memory subsystem.
This effect should be expected to reach a steady state above a certain number of pixels.
In addition to that, a separate vector needs to be allocated for every pixel to store the events.
This allocation cost could grow linearly with the pixel count.

\begin{table}
  \centering
  \begin{tabular}{c|rrr}
    Instrument & SANS2D & MERLIN & WISH\\
    Pixel count & 122888 & 286729 & 778245\\
    \hline
    $10^5$ events in 14 frames & 1379 & 915 & 400 \\
    $10^6$ events in 14 frames & 284 & 93 & 38 \\
    $10^7$ events in 14 frames & 23 & 8 & 3 \\
  \end{tabular}
  \caption{\label{tab:consumption-rate}Number of consumed frames per second. For ESS there are 14 frames (pulses) per second, i.e., we require a consumption rate of at least 14. The update timeout is set to $1~\mathrm{s}$.}
\end{table}

By increasing the update timeout, which was set to one second in Tab.~\ref{tab:consumption-rate}, the consumption rate can be improved moderately.
A couple of other optimization also appear to be possible.
On the other hand, live display of the raw data using the \verb|InstrumentView|, (simplified) data reduction, and live display of the reduced data add extra overhead.
Combining these opposite effects, which --- according to preliminary benchmarks --- all appear to be in the order of a $2\times$ change, leads us to conclude that the numbers given in Tab.~\ref{tab:consumption-rate} are a reasonable estimate.
That is, without having access to much more detailed workflows and instrument models we have the crude estimate

\begin{align}
  R_{\text{live}} &= \frac{N_{\text{frame}}^{\text{max}}}{14} = \frac{3\cdot10^{13}~\mathrm{s}}{14\Fevent\Nspec} = \frac{2\cdot10^{12}~\mathrm{s}}{\Fevent\Nspec},
\end{align}
where $N_{\text{frame}}^{\text{max}}$ is the maximum number of consumed frames as in Tab.~\ref{tab:consumption-rate}, 14 is the number of frames per second at ESS, and $\Fevent$ is the event rate.
We require $R_{\text{live}} > 1$ to be able to support live reduction without MPI.
In Fig.~\ref{fig:live-reduction-no-mpi} we show the implications for the first ESS instruments, depending on the accelerator power.
With the exception of \dream and possibly \magic, we seem to be able to handle most cases for most instruments even at higher accelerator power.
However, we will definitely require MPI support in live reduction for \dream from day one.

\begin{figure}
  \centering
\includegraphics[height=0.8317\textheight]{live-reduction-no-mpi.pdf}
\caption{\label{fig:live-reduction-no-mpi}Reduction speed ratio $R_{\text{live}}$ for the first ESS instruments.
With increasing accelerator power, $\Fevent$ increases and $R_{\text{live}}$ decreases.
We require $R_{\text{live}} > 1$ to be able to handle the data.
The plot range has been chosen accordingly --- a graph hitting the lower horizontal axis implies that the rate cannot be handled anymore.
For \bifrost the pixel count is very low and we are not confident that we can extrapolate the scaling benchmarks from Tab.~\ref{tab:consumption-rate} this far.
We have therefore excluded the panel for \bifrost but expect that the rate can be handled even at a power of $5~\mathrm{MW}$.
The imaging beamline \odin is a special case and has thus also been excluded from this figure.
}
\end{figure}

There is another aspect that we have not covered yet.
For experiments with very high throughput, i.e., quick changes of sample or sample-environment parameters, we would require additional complexity in the live reduction.
This could include more advanced filtering, display and comparison of multiple results at the same time, and overhead from frequent transitions to a new run.
We have not yet started development of these features so benchmarks are not available.
That being said, high throughput experiments are not likely to required in the early days.
Furthermore, UX is likely to be a bigger issue than performance in this case.
We consider it unlikely that this scenario will cause a significant overall difference in the required hardware by the time it will be required.

\section{Interactive reduction}

As discussed in Sec.~\ref{sec:threading-and-mpi-support} there is not MPI support for the GUI or interactive reduction in general and therefore the is limit scaling the number of available cores.
Generally we do not expect good scaling far beyond 10 cores.
For reductions that are too slow in that case there are several options that can be used in practice:
\begin{itemize}
  \item Use slow interactive reduction for a single run/sample, apply resulting workflow to other samples using script-based reduction on the cluster.
  \item Use interactive reduction using a reduced or compressed set of data, e.g., by loading only every $N$-th pulse or by compressing events, apply resulting workflow to other samples using script-based reduction on the cluster.
  \item Use slow interactive reduction and use different sessions to do something else in the meantime, such as reducing another sample.
\end{itemize}
With exception of the second option, there is a minimum amount of required total RAM.
We give example requirements for various instrument configurations in Tab.~\ref{tab:total-ram}.

\begin{table}
  \centering
  \begin{tabular}{lrlr}
    Instrument & Pixels & Mode & Typical minimum RAM\\
    \hline
    \multirow{12}{*}{\loki}
    & \multirow{6}{*}{750000}
    & 3m-high-flux    & \\
    && 3m-small-sample & \\
    && 5m-high-flux    & \\
    && 5m-small-sample & \\
    && 8m-high-flux    & \\
    && 8m-small-sample & \\\cline{2-4}
    & \multirow{6}{*}{1500000}
    & 3m-high-flux    & \\
    && 3m-small-sample & \\
    && 5m-high-flux    & \\
    && 5m-small-sample & \\
    && 8m-high-flux    & \\
    && 8m-small-sample & \\
    \hline
  \end{tabular}
  \caption{\label{tab:total-ram}
Approximate total RAM requirement for data reduction.
}
\end{table}

Instrument scientist expect that typically 5 interactive sessions would be required per instrument at any given time.
This figure is to be combined with requirements for data analysis since typically the interactive sessions would be used for both, data reduction and data analysis.
For data reduction, a lot of the time Mantid will actually be idle or unused.
Thus simply adding the required 5 sessions to the required numbers for data analysis is likely to overestimate the required resources.

\section{Script-based reduction}

list of benchmark runs/results and considered instrument
example tables of required hardware (for some set of parameters)

appendix: usage instructions for scripts

\section{Hardware components}

\subsection{RAM}

Some but not all reductions require a lot of RAM.
Therefore, distinguishing between \emph{normal} and \emph{high-mem} machine configurations probably makes sense.
TODO: How many high-mem nodes are required, compared to normal? What is normal, what is high?

Typically faster RAM is more important than more cores (detailed benchmarks pending)?

\subsection{Network}

For data reduction, Mantid does not put particularly high demands on the interconnect between nodes.
The most important factor is the speed of the connection to the file system.
During file system reads and writes there will also be significant communication between nodes.
Non-I/O algorithms in Mantid typically have either no communication or communication with only moderate latency and bandwidth requirements.
While Ethernet is not sufficient, our current feeling is that any Infiniband network is, even if it is an older revision.

\subsection{Disk I/O}

effect of I/O bandwidth, in particular limits on reduction speed.


\section{Interpretation}




\appendix

\section{Related documentation}

\begin{itemize}
  \item Repository containing scripts to produce data in this document as well as the source for this document:\\
    \url{https://github.com/DMSC-Instrument-Data/data-reduction-hardware-requirements}
  \item Initial thoughts and plan for estimating hardware requirements:\\
    \url{https://github.com/DMSC-Instrument-Data/documents/blob/master/mantid/hardware-requirements/plan.md}
  \item Notes from meetings with instrument responsibles:\\
    \url{https://confluence.esss.lu.se/display/DAM/Data+Reduction}
  \item Benchmarks for live reduction:\\
    \url{https://github.com/DMSC-Instrument-Data/documents/blob/master/investigations/Live%20Reduction/LiveReductionInvestigation.md}
  \item Benchmarks for event filtering:\\
    \url{https://github.com/DMSC-Instrument-Data/data-reduction-hardware-requirements/blob/master/benchmarks/nvaytet/summary.md}
  \item \odin data rate report:\\
    \url{https://confluence.esss.lu.se/download/attachments/274116159/report_ODIN_data_rate_volume.pdf?api=v2}
\end{itemize}

\section{Motivation of the performance master equation}
\label{app:master_eq}

The rationale for Eq.~\eqref{eq:master} is as follows:

\begin{itemize}
  \item For the vast majority of algorithms used in data reduction, all spectra are treated independently.
  Thus there is a linear term in $\Nspec$ or $\Nbin$, but no higher order terms, and there is perfect scaling with $\Ncore$.
\item Events in Mantid are stored with their respective spectrum.
  Strictly speaking, we should thus include a term
  \begin{align}
    \Treduction^{\text{nonlinear}} &= \sum_{i=1}^{\Nspec} (N_{event,i} t_{event,linear} + N_{event,i} log N_{event,i} \Tevent^{\text{NlogN}} + ...),
    \intertext{
      i.e., a different term for each spectrum, depending on the number of events in that spectrum, and non-linear term, such as for sorting events.
    However, events are usually spread over many spectra, so we can approximate}
    N_{event,i} &\approx \Nevent/\Nspec.
    \intertext{We obtain}
    \Treduction^{\text{nonlinear}} &= \sum_{i=1}^{\Nspec} (\Nevent^i \Tevent^{\text{linear}} + \Nevent^i \log \Nevent^i \Tevent^{\text{NlogN}} + ...)\\
    &\approx \sum_{i=1}^{\Nspec} (\frac{\Nevent}{\Nspec}  \Tevent^{\text{linear}} + \Nevent/\Nspec \log \frac{\Nevent}{\Nspec}  \Tevent^{\text{NlogN}} + ...)\\
    &= \Nevent\Tevent^{\text{linear}} + \frac{\Nevent}{\Nspec} \sum_{i=1}^{\Nspec} (\log \frac{\Nevent}{\Nspec}  \Tevent^{\text{NlogN}} + ...)\\
    &= \Nevent\Tevent^{\text{linear}} + \Nevent (\log \frac{\Nevent}{\Nspec}  \Tevent^{\text{NlogN}} + ...).
    \intertext{The term in parenthesis depends on $\log (\Nevent/\Nspec)$ and \emph{not} $\log \Nevent$ (similar for higher order terms).
  These terms are thus typically small and it seems reasonable to absorb them into the linear term}
    &\approx \Nevent*\Tevent.
  \end{align}
  If instead events are peaked in a subset of spectra we approximate $N_{event,i} ~ (0 \text{ or } \Nevent/N_{\text{peak}}$, the approximation works in a similar way.
\item Loading large event files is a significant contribution to the overall reduction time.
  While the exact scaling of the new parallel event loader is unknown (no adequate parallel file system available), there is definitely a linearly scaling contribution that scales well with the number of cores and is thus already described by the $\Nevent*\Tevent$ term.
  In addition to that, the other major factor is given by the upper limit of file system bandwidth.
  Basically, this is a limit to the number of events that can be loaded per second.
  This will also depend on whether or not compression is used in NeXus files.
  We model this limit in the equation with the term $\Nevent/\Bmax$.
  In case a parallel file system provides an bandwidth that is much higher on average than what was benchmarked for a local SSD, we may need to include a different term that captures limited scaling of the parallel loader.
\item The time for reducing a spectrum will often depend linearly on the bin count.
  Many instrument can adjust their resolution, usually by sacrificing brightness.
  Thus the bin count will be fixed for a given run but can vary between runs within a instrument-specific range.
\end{itemize}

\section{Instrument parameters}

\subsection{\estia}

From the original proposal:

\begin{itemize}
  \item Wavelength range is $5~\angstrom$ to $9.4~\angstrom$ with an intrinsic resolution of 2\% and 4\%, respectively.
    In the simplest case with constant bin width this yields $10\times(9.4-5.0)/(0.02\cdot5.0) = 450$ bins, where the factor of 10 is the aforementioned rule of thumb to obtain a required bin count from a resolution.
\end{itemize}

\subsection{\magic}

\begin{itemize}
  \item The shortest pulse from the pulse-shaping chopper is $100~\mathrm{\mu s}$ yielding $10\times71~\mathrm{ms}/100~\mathrm{\mu s} = 7100$ bins, where $71~\mathrm{ms}$ is the time to the next pulse.
    While there is a high-flux option, my understanding is that this depends on collimation while the pulse-shaping is unchanged, i.e., we always have this pulse length.
\end{itemize}

\subsection{\dream}

\begin{itemize}
  \item The (useful) range of the pulse shaping is $10~\mathrm{\mu s}$ to $500~\mathrm{\mu s}$.
    We thus obtain $10\times71~\mathrm{ms}/10~\mathrm{\mu s} = 71000$ bins for the highest resolution and $10\times71~\mathrm{ms}/500~\mathrm{\mu s} = 1420$ for the lowest resolution.
\end{itemize}

\subsection{\beer}

\begin{itemize}
  \item According to the original proposal, resolution settings range from 0.1\% to 1\%. The wavelength band is $1.7~\angstrom$ but there are chopper settings to double it using pulse skipping. For a wavelength band centered at $2~\angstrom$ this yields bins counts of $10\times1.7~\angstrom/(0.01\cdot2~\angstrom) = 850$ for the low resolution setting and $10\times1.7~\angstrom/(0.001\cdot2~\angstrom) = 8500$ for the highest resolution.
\end{itemize}

%We list possible and ongoing mitigations in Tab.~\ref{tab:unknown_factors}.

%\begin{table}
%    \small
%\begin{tabularx}{\textwidth}{lXl}
%    \hline
%    what & potential solution & status \\
%    \hline
%    event rate & work based on highest estimates & ongoing \\
%    available hardware & communicate (to instrument teams?) that there has to be a trade off between hardware cost and options in reduction, discuss with Sune & to do \\
%    3D detectors & redesign Mantid instrument/geometry code, get more details from detector group & to do \\
%    Mantid optimizations & depends on instrument/geometry redesign & blocked\\
%    load/stream performance & do more benchmarks, work on improvements & to do \\
%    complexer instruments & discuss details with instrument scientists from early on & started \\
%    complexer science cases & discuss details with instrument scientists from early on & to do \\
%    event mode & discuss options with Mantid team and instrument scientists & started \\
%    monitors &  discuss options with Mantid team and instrument scientists & to do \\
%    \hline
%\end{tabularx}
%\caption{\label{tab:unknown_factors}Unknown factors and mitigations.}
%\end{table}


\end{document}
